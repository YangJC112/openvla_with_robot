{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuangzhi/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-19 18:42:06.855894: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-19 18:42:06.855985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-19 18:42:06.857157: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-19 18:42:06.863034: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-19 18:42:07.530708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers import AutoConfig, AutoImageProcessor\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "import wandb\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.util.data_utils import PaddedCollatorForActionPrediction\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset\n",
    "from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "\n",
    "from prismatic.extern.hf.configuration_prismatic import OpenVLAConfig\n",
    "from prismatic.extern.hf.modeling_prismatic import OpenVLAForActionPrediction\n",
    "from prismatic.extern.hf.processing_prismatic import PrismaticImageProcessor, PrismaticProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = None\n",
    "if cfg.use_quantization:\n",
    "    assert cfg.use_lora, \"Quantized training only supported for LoRA fine-tuning!\"\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "# Register OpenVLA model to HF Auto Classes (not needed if the model is on HF Hub)\n",
    "AutoConfig.register(\"openvla\", OpenVLAConfig)\n",
    "AutoImageProcessor.register(OpenVLAConfig, PrismaticImageProcessor)\n",
    "AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n",
    "AutoModelForVision2Seq.register(OpenVLAConfig, OpenVLAForActionPrediction)\n",
    "\n",
    "# Load OpenVLA Processor and Model using HF AutoClasses\n",
    "processor = AutoProcessor.from_pretrained(cfg.vla_path, trust_remote_code=True)\n",
    "# vla = AutoModelForVision2Seq.from_pretrained(\n",
    "#     cfg.vla_path,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     quantization_config=quantization_config,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# Create Action Tokenizer\n",
    "action_tokenizer = ActionTokenizer(processor.tokenizer)\n",
    "\n",
    "batch_transform = RLDSBatchTransform(\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder if \"v01\" not in cfg.vla_path else VicunaV15ChatPromptBuilder,\n",
    ")\n",
    "\n",
    "# vla = AutoModelForVision2Seq.from_pretrained(\n",
    "#     cfg.vla_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True, trust_remote_code=True\n",
    "# )\n",
    "\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    cfg.vla_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"base vla from pretrained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9e027642ba48e7825aeb0a48be0c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install minimal dependencies (`torch`, `transformers`, `timm`, `tokenizers`, ...)\n",
    "# > pip install -r https://raw.githubusercontent.com/openvla/openvla/main/requirements-min.txt\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load Processor & VLA`\n",
    "# model_path = r\"/root/autodl-tmp/openvla/myScripts/autodl-tmp/huggingface_models\"\n",
    "# model_path = r\"/root/autodl-tmp/openvla/myScripts/runs/huggingface_models+austin_buds_dataset_converted_externally_to_rlds+b2+lr-0.0005+lora-r32+dropout-0.0\"\n",
    "#! export HF_ENDPOINT=https://hf-mirror.com\n",
    "#! huggingface-cli download --resume-download openvla/openvla-7b --local-dir openvla7b_huggingfacemodel\n",
    "# model_path = r\"/home/chuangzhi/zhq/yjc/openvla7b_huggingfacemodel\"\n",
    "model_path = r\"/home/chuangzhi/zhq/yjc/runs/openvla7b_huggingfacemodel+libero_spatial_no_noops+b2+lr-0.0005+lora-r32+dropout-0.0+example_dataset+b1+lr-0.0005+lora-r32+dropout-0.0\"\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_path, \n",
    "    attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    trust_remote_code=True\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "# Grab image input & format prompt\n",
    "# image: Image.Image = get_from_camera(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab image input & format prompt\n",
    "image_path = r'/root/autodl-tmp/openvla/myScripts/1.jpg' # 替换为你的图像路径\n",
    "image = Image.open(image_path)  # 加载图像\n",
    "\n",
    "prompt = \"In: What action should the robot take to {<INSTRUCTION>}?\\nOut:\"\n",
    "\n",
    "# Predict Action (7-DoF; un-normalize for BridgeData V2)\n",
    "inputs = processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': device(type='cuda', index=0), 'attention_mask': device(type='cuda', index=0), 'pixel_values': device(type='cuda', index=0)}\n"
     ]
    }
   ],
   "source": [
    "devices = {key: value.device if isinstance(value, torch.Tensor) else \"Not a Tensor\" for key, value in inputs.items()}\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.device' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mdevice()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.device' object is not callable"
     ]
    }
   ],
   "source": [
    "# inputs['input_ids'].device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1118,  0.2139,  0.3867,  0.5742],\n",
       "          [-0.0089,  0.3008,  0.4551,  0.6250],\n",
       "          [ 0.1455,  0.4023,  0.5391,  0.6953],\n",
       "          [ 0.3008,  0.4883,  0.6250,  0.7617]],\n",
       "\n",
       "         [[-0.1582,  0.0869,  0.2969,  0.4023],\n",
       "          [-0.0359,  0.1748,  0.3496,  0.4727],\n",
       "          [ 0.0869,  0.2793,  0.4375,  0.5430],\n",
       "          [ 0.2100,  0.2969,  0.4727,  0.5586]]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'][ :, :2,:4,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00538307  0.00766987 -0.00335915 -0.02568265  0.04945926  0.10147657\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)\n",
    "print(action)\n",
    "# # Execute...\n",
    "# robot.act(action, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuangzhi/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-22 16:55:33.800944: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-22 16:55:33.801044: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-22 16:55:33.802274: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-22 16:55:33.808140: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-22 16:55:34.506983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from transformers import AutoConfig, AutoImageProcessor\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from typing import Type, Any\n",
    "from prismatic.extern.hf.configuration_prismatic import OpenVLAConfig\n",
    "from prismatic.extern.hf.modeling_prismatic import OpenVLAForActionPrediction\n",
    "from prismatic.extern.hf.processing_prismatic import PrismaticImageProcessor, PrismaticProcessor\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.util.data_utils import PaddedCollatorForActionPrediction\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset\n",
    "from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "# 注册 OpenVLA 模型到 HF Auto Classes\n",
    "AutoConfig.register(\"openvla\", OpenVLAConfig)\n",
    "AutoImageProcessor.register(OpenVLAConfig, PrismaticImageProcessor)\n",
    "AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n",
    "AutoModelForVision2Seq.register(OpenVLAConfig, OpenVLAForActionPrediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Union, Any\n",
    "import json\n",
    "import os\n",
    "\n",
    "class OpenVLAInference:\n",
    "    def __init__(self, model_path, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        初始化 OpenVLA 推理器\n",
    "        \n",
    "        参数:\n",
    "            model_path: 模型路径 (可以是 HuggingFace Hub 路径或本地路径)\n",
    "            device: 推理设备 ('cuda' 或 'cpu')\n",
    "        \"\"\"\n",
    "        self.device = device if torch.cuda.is_available() and device.startswith(\"cuda\") else \"cpu\"\n",
    "        dataset_statistics_path = os.path.join(model_path, \"dataset_statistics.json\")\n",
    "        if os.path.isfile(dataset_statistics_path):\n",
    "            with open(dataset_statistics_path, \"r\") as f:\n",
    "                norm_stats = json.load(f)\n",
    "            self.norm_stats = norm_stats\n",
    "        self.unnorm_key = \"example_dataset\"\n",
    "        assert self.unnorm_key in self.norm_stats, f\"Action un-norm key {self.unnorm_key} not found in VLA `norm_stats`!\"\n",
    "        # 加载处理器和模型\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForVision2Seq.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        ).to(self.device)\n",
    "        self.model.norm_stats = self.norm_stats\n",
    "        # 创建 action tokenizer\n",
    "        self.action_tokenizer = ActionTokenizer(self.processor.tokenizer)\n",
    "        \n",
    "        # 根据模型版本选择 prompt builder\n",
    "        self.prompt_builder_cls = (\n",
    "            PurePromptBuilder if \"v01\" not in model_path \n",
    "            else VicunaV15ChatPromptBuilder\n",
    "        )\n",
    "        \n",
    "        # 设置模型为评估模式\n",
    "        self.model.eval()\n",
    "    \n",
    "    def _build_prompt(self, text_instruction: str) -> str:\n",
    "        \"\"\"\n",
    "        构建推理时使用的 prompt (直接实现原RLDSBatchTransform的逻辑)\n",
    "        \n",
    "        参数:\n",
    "            text_instruction: 文本指令 (如 \"wipe the table\")\n",
    "            \n",
    "        返回:\n",
    "            格式化后的 prompt 文本\n",
    "        \"\"\"\n",
    "        # 初始化 prompt builder\n",
    "        self.prompt_builder = self.prompt_builder_cls(\"openvla\")\n",
    "        # print(\"self.prompt_builder.turn_count\",self.prompt_builder.turn_count)\n",
    "        conversation = [\n",
    "            {\"from\": \"human\", \"value\":f\"What action should the robot take to {text_instruction.lower()}?\"},\n",
    "            # {\"from\": \"gpt\", \"value\":\"\"},\n",
    "        ]\n",
    "        # 添加对话轮次 (只包含人类指令部分)\n",
    "        for turn in conversation:\n",
    "            self.prompt_builder.add_turn(turn[\"from\"], turn[\"value\"])\n",
    "        # print(\"self.prompt_builder.turn_count\",self.prompt_builder.turn_count)\n",
    "        self.turn_count = self.prompt_builder.turn_count\n",
    "        return self.prompt_builder.get_prompt()\n",
    "    \n",
    "    def preprocess_inputs(self, text_prompt: str, image: Any) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        预处理输入文本和图像\n",
    "        \n",
    "        参数:\n",
    "            text_prompt: 文本指令\n",
    "            image: PIL.Image 对象或图像路径\n",
    "            \n",
    "        返回:\n",
    "            处理后的模型输入字典\n",
    "        \"\"\"\n",
    "        # 如果 image 是路径，则加载图像\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "        \n",
    "        # 构建 prompt\n",
    "        prompt_text = self._build_prompt(text_prompt)\n",
    "        self.input_ids = self.processor.tokenizer(prompt_text, add_special_tokens=True).input_ids\n",
    "        self.input_ids = torch.tensor(self.input_ids)\n",
    "        # 使用处理器处理输入\n",
    "        inputs = self.processor(\n",
    "            text=prompt_text,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # 将输入移动到设备\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def generate_actions(self, text_prompt: str, image: Any, \n",
    "                       max_new_tokens: int = 512, \n",
    "                       temperature: float = 0) -> tuple:\n",
    "        \"\"\"\n",
    "        生成动作序列\n",
    "        \n",
    "        参数:\n",
    "            text_prompt: 文本指令\n",
    "            image: PIL.Image 对象或图像路径\n",
    "            max_new_tokens: 最大生成 token 数\n",
    "            temperature: 采样温度\n",
    "            \n",
    "        返回:\n",
    "            tuple: (action_sequence, decoded_actions)\n",
    "                - action_sequence: 动作序列 (numpy 数组)\n",
    "                - decoded_actions: 解码后的动作 (人类可读格式)\n",
    "        \"\"\"\n",
    "        # 预处理输入\n",
    "        inputs = self.preprocess_inputs(text_prompt, image)\n",
    "        \n",
    "        # 生成动作 token\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            self.output = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True if temperature > 0 else False,\n",
    "                pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "            )\n",
    "            self.action = self.model.predict_action(**inputs,\n",
    "                                               unnorm_key=self.unnorm_key,\n",
    "                                               temperature=temperature,\n",
    "                                               do_sample=True if temperature > 0 else False,\n",
    "                                               )\n",
    "        mask = self.output > self.action_tokenizer.action_token_begin_idx\n",
    "        # 解码动作 token\n",
    "        action_tokens = self.output[mask].cpu().numpy()\n",
    "        action_sequence = self.action_tokenizer.decode_token_ids_to_actions(action_tokens)\n",
    "        \n",
    "        # 获取人类可读的动作描述\n",
    "        # decoded_actions = self.action_tokenizer.decode_actions_to_readable(action_sequence)\n",
    "        \n",
    "        return action_sequence\n",
    "    \n",
    "    def __call__(self, text_prompt: str, image: Any, \n",
    "                max_new_tokens: int = 512, \n",
    "                temperature: float = 0) -> tuple:\n",
    "        \"\"\"便捷调用方法\"\"\"\n",
    "        return self.generate_actions(text_prompt, image, max_new_tokens, temperature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda:4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 使用示例\n",
    "# !CUDA_VISIBLE_DEVICES=1\n",
    "vla_path = r\"/home/chuangzhi/zhq/yjc/runs/openvla7b_huggingfacemodel+libero_spatial_no_noops+b2+lr-0.0005+lora-r32+dropout-0.0+example_dataset+b1+lr-0.0005+lora-r32+dropout-0.0\"\n",
    "# vla_path = r\"/home/chuangzhi/zhq/yjc/runs/openvla7b_huggingfacemodel+libero_spatial_no_noops+b2+lr-0.0005+lora-r32+dropout-0.0\"\n",
    "# 初始化推理器 - 替换为你的模型路径  # 可以是本地路径或 HuggingFace Hub 路径\n",
    "vla_inference = OpenVLAInference(vla_path,device=\"cuda:4\")\n",
    "vla_inference.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['example_dataset'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla_inference.norm_stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.prompt_builder.turn_count 0\n",
      "self.prompt_builder.turn_count 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuangzhi/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OpenVLAInference' object has no attribute 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/chuangzhi/zhq/yjc/myScripts/0.png\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 你的图像路径\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 生成动作\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m action_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mvla_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_instruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 打印结果\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Action Sequence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, action_sequence)\n",
      "Cell \u001b[0;32mIn[6], line 153\u001b[0m, in \u001b[0;36mOpenVLAInference.__call__\u001b[0;34m(self, text_prompt, image, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_prompt: \u001b[38;5;28mstr\u001b[39m, image: Any, \n\u001b[1;32m    150\u001b[0m             max_new_tokens: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, \n\u001b[1;32m    151\u001b[0m             temperature: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"便捷调用方法\"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 126\u001b[0m, in \u001b[0;36mOpenVLAInference.generate_actions\u001b[0;34m(self, text_prompt, image, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# 生成动作 token\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m    128\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[1;32m    129\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    130\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    131\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m    132\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict_action(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m    135\u001b[0m                                        unnorm_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnorm_key,\n\u001b[1;32m    136\u001b[0m                                        temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    137\u001b[0m                                        do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    138\u001b[0m                                        )\n\u001b[1;32m    139\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_tokenizer\u001b[38;5;241m.\u001b[39maction_token_begin_idx\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenVLAInference' object has no attribute 'self'"
     ]
    }
   ],
   "source": [
    "# 示例输入\n",
    "text_instruction = \"Move the drinking glass to the basket.\"  # 注意使用小写，与训练时一致\n",
    "image_path = \"/home/chuangzhi/zhq/yjc/myScripts/0.png\"  # 你的图像路径\n",
    "\n",
    "# 生成动作\n",
    "action_sequence = vla_inference(text_instruction, image_path)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Generated Action Sequence:\", action_sequence)\n",
    "print(f'\"action\" 序列长度为:{len(action_sequence)}')\n",
    "print(vla_inference.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.prompt_builder.turn_count 0\n",
      "self.prompt_builder.turn_count 1\n",
      "Generated Action Sequence: [0.90980392 0.24313725 0.         0.09411765 0.10980392 0.19607843\n",
      " 0.99607843]\n",
      "\"action\" 序列长度为:7\n"
     ]
    }
   ],
   "source": [
    "# 示例输入\n",
    "text_instruction = \"Move the drinking glass to the basket.\"  # 注意使用小写，与训练时一致\n",
    "image_path = \"/home/chuangzhi/zhq/yjc/myScripts/0.png\"  # 你的图像路径\n",
    "\n",
    "# 生成动作\n",
    "action_sequence = vla_inference(text_instruction, image_path)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Generated Action Sequence:\", action_sequence)\n",
    "print(f'\"action\" 序列长度为:{len(action_sequence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla_inference.prompt_builder.turn_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla_inference.turn_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla_inference.processor.tokenizer.pad_token_id, vla_inference.processor.tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_builder.turn_count 0\n",
      "prompt_builder.turn_count 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   512, 29901,  1724,  3158,   881,   278, 19964,  2125,   304,\n",
       "           4337,   278, 13748,   292, 12917,   304,   278, 25972, 29889, 29973,\n",
       "             13,  3744, 29901, 29871]], device='cuda:2'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "        device='cuda:2'),\n",
       " 'pixel_values': tensor([[[[-1.6406, -1.7578, -2.0000,  ..., -0.1631, -0.1631, -0.1465],\n",
       "           [-1.6562, -1.6875, -1.9688,  ..., -0.1631, -0.1465, -0.1465],\n",
       "           [-1.7109, -1.6562, -1.8984,  ..., -0.1807, -0.1807, -0.1631],\n",
       "           ...,\n",
       "           [ 0.6094,  0.5898,  0.6250,  ...,  0.7109,  0.7109,  0.7109],\n",
       "           [ 0.6602,  0.6250,  0.6758,  ...,  0.7461,  0.7461,  0.7109],\n",
       "           [ 0.6602,  0.6445,  0.6602,  ...,  0.7109,  0.7305,  0.6953]],\n",
       " \n",
       "          [[-1.5234, -1.6641, -1.9141,  ...,  0.1572,  0.1396,  0.1221],\n",
       "           [-1.5469, -1.5938, -1.8750,  ...,  0.1572,  0.1748,  0.1748],\n",
       "           [-1.6484, -1.5781, -1.7891,  ...,  0.1396,  0.1396,  0.1572],\n",
       "           ...,\n",
       "           [ 0.9297,  0.9297,  0.9297,  ...,  0.9453,  0.9453,  0.9453],\n",
       "           [ 0.9297,  0.8945,  0.9453,  ...,  0.9805,  0.9805,  0.9453],\n",
       "           [ 0.9453,  0.9102,  0.9297,  ...,  0.9453,  0.9648,  0.9297]],\n",
       " \n",
       "          [[-1.4922, -1.5625, -1.7578,  ...,  0.3047,  0.2871,  0.2871],\n",
       "           [-1.5156, -1.5156, -1.7734,  ...,  0.3047,  0.3223,  0.3223],\n",
       "           [-1.6016, -1.5156, -1.7031,  ...,  0.3223,  0.3223,  0.3379],\n",
       "           ...,\n",
       "           [ 1.1953,  1.1953,  1.1953,  ...,  1.2500,  1.2500,  1.2500],\n",
       "           [ 1.2109,  1.1797,  1.2266,  ...,  1.2812,  1.2812,  1.2500],\n",
       "           [ 1.1797,  1.1797,  1.2109,  ...,  1.2500,  1.2656,  1.2266]],\n",
       " \n",
       "          [[-0.7812, -0.8359, -0.9453,  ..., -0.1060, -0.1060, -0.0981],\n",
       "           [-0.7891, -0.8047, -0.9297,  ..., -0.1060, -0.0981, -0.0981],\n",
       "           [-0.8125, -0.7891, -0.8984,  ..., -0.1138, -0.1138, -0.1060],\n",
       "           ...,\n",
       "           [ 0.2471,  0.2393,  0.2559,  ...,  0.2949,  0.2949,  0.2949],\n",
       "           [ 0.2715,  0.2559,  0.2793,  ...,  0.3105,  0.3105,  0.2949],\n",
       "           [ 0.2715,  0.2637,  0.2715,  ...,  0.2949,  0.3027,  0.2871]],\n",
       " \n",
       "          [[-0.7734, -0.8359, -0.9453,  ..., -0.0197, -0.0275, -0.0354],\n",
       "           [-0.7812, -0.8047, -0.9297,  ..., -0.0197, -0.0118, -0.0118],\n",
       "           [-0.8281, -0.7969, -0.8906,  ..., -0.0275, -0.0275, -0.0197],\n",
       "           ...,\n",
       "           [ 0.3262,  0.3262,  0.3262,  ...,  0.3340,  0.3340,  0.3340],\n",
       "           [ 0.3262,  0.3105,  0.3340,  ...,  0.3496,  0.3496,  0.3340],\n",
       "           [ 0.3340,  0.3184,  0.3262,  ...,  0.3340,  0.3418,  0.3262]],\n",
       " \n",
       "          [[-0.8594, -0.8906, -0.9766,  ..., -0.0510, -0.0588, -0.0588],\n",
       "           [-0.8672, -0.8672, -0.9844,  ..., -0.0510, -0.0432, -0.0432],\n",
       "           [-0.9062, -0.8672, -0.9531,  ..., -0.0432, -0.0432, -0.0354],\n",
       "           ...,\n",
       "           [ 0.3496,  0.3496,  0.3496,  ...,  0.3730,  0.3730,  0.3730],\n",
       "           [ 0.3574,  0.3418,  0.3652,  ...,  0.3887,  0.3887,  0.3730],\n",
       "           [ 0.3418,  0.3418,  0.3574,  ...,  0.3730,  0.3809,  0.3652]]]],\n",
       "        device='cuda:2', dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = vla_inference.preprocess_inputs(text_instruction, img)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   512, 29901,  1724,  3158,   881,   278, 19964,  2125,   304,\n",
       "         4337,   278, 13748,   292, 12917,   304,   278, 25972, 29889, 29973,\n",
       "           13,  3744, 29901])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla_inference.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'pixel_values']),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "        device='cuda:4'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys(),inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In: What action should the robot take to move the drinking glass to the basket?\\nOut:'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla_inference._build_prompt(text_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.prompt_builder.turn_count 0\n",
      "self.prompt_builder.turn_count 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In: What action should the robot take to move the drinking glass to the basket.?\\nOut: '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla_inference._build_prompt(text_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31743"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla_inference.action_tokenizer.action_token_begin_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m \u001b[38;5;241m>\u001b[39m vla_inference\u001b[38;5;241m.\u001b[39maction_tokenizer\u001b[38;5;241m.\u001b[39maction_token_begin_idx\n\u001b[1;32m      3\u001b[0m output[mask]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   512, 29901,  1724,  3158,   881,   278, 19964,  2125,   304,\n",
       "          4337,   278, 13748,   292, 12917,   304,   278, 25972, 29889, 29973,\n",
       "            13,  3744, 29901, 31756, 31841, 31872, 31860, 31880, 31867, 31872,\n",
       "         31744,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    output = vla_inference.model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=vla_inference.processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=vla_inference.processor.tokenizer.eos_token_id,\n",
    "    )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuangzhi/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mask = output > vla_inference.action_tokenizer.action_token_begin_idx\n",
    "\n",
    "output[mask].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(action_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
