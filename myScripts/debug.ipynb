{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/openvla\n",
      "['/root/autodl-tmp/openvla/myScripts', '/root/miniconda3/envs/openvla/lib/python310.zip', '/root/miniconda3/envs/openvla/lib/python3.10', '/root/miniconda3/envs/openvla/lib/python3.10/lib-dynload', '', '/root/miniconda3/envs/openvla/lib/python3.10/site-packages', '__editable__.openvla-0.0.3.finder.__path_hook__', '/root/autodl-tmp/LIBERO', '/root/autodl-tmp/openvla']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 10:52:06.765628: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-28 10:52:06.824582: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-28 10:52:06.824610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-28 10:52:06.826555: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-28 10:52:06.841985: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-28 10:52:08.200791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到 sys.path\n",
    "# 获取当前工作目录\n",
    "current_dir = get_ipython().run_line_magic('pwd', '')\n",
    "# 假设项目根目录是当前工作目录的上两级目录\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "sys.path.append(project_root)\n",
    "print(project_root)\n",
    "print(sys.path)\n",
    "import os\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers import AutoConfig, AutoImageProcessor\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "import wandb\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.util.data_utils import PaddedCollatorForActionPrediction\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset\n",
    "from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "\n",
    "from prismatic.extern.hf.configuration_prismatic import OpenVLAConfig\n",
    "from prismatic.extern.hf.modeling_prismatic import OpenVLAForActionPrediction\n",
    "from prismatic.extern.hf.processing_prismatic import PrismaticImageProcessor, PrismaticProcessor\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vla_path: str = \"openvla/openvla-7b\",\n",
    "        data_root_dir: Path = Path(\"datasets/open-x-embodiment\"),\n",
    "        dataset_name: str = \"droid_wipe\",\n",
    "        run_root_dir: Path = Path(\"runs\"),\n",
    "        adapter_tmp_dir: Path = Path(\"adapter-tmp\"),\n",
    "        batch_size: int = 2,\n",
    "        max_steps: int = 400,\n",
    "        save_steps: int = 200,\n",
    "        learning_rate: float = 5e-4,\n",
    "        grad_accumulation_steps: int = 1,\n",
    "        image_aug: bool = True,\n",
    "        shuffle_buffer_size: int = 100_000,\n",
    "        save_latest_checkpoint_only: bool = False,\n",
    "        use_lora: bool = True,\n",
    "        lora_rank: int = 32,\n",
    "        lora_dropout: float = 0.0,\n",
    "        use_quantization: bool = True,\n",
    "        wandb_project: str = \"openvla\",\n",
    "        wandb_entity: str = \"stanford-voltron\",\n",
    "        run_id_note: Optional[str] = None,\n",
    "    ):\n",
    "    # fmt: on\n",
    "        self.vla_path = vla_path\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.dataset_name = dataset_name\n",
    "        self.run_root_dir = run_root_dir\n",
    "        self.adapter_tmp_dir = adapter_tmp_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.max_steps = max_steps\n",
    "        self.save_steps = save_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_accumulation_steps = grad_accumulation_steps\n",
    "        self.image_aug = image_aug\n",
    "        self.shuffle_buffer_size = shuffle_buffer_size\n",
    "        self.save_latest_checkpoint_only = save_latest_checkpoint_only\n",
    "        self.use_lora = use_lora\n",
    "        self.lora_rank = lora_rank\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.use_quantization = use_quantization\n",
    "        self.wandb_project = wandb_project\n",
    "        self.wandb_entity = wandb_entity\n",
    "        self.run_id_note = run_id_note\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning with the following configuration:\n",
      "vla_path: C:\\Users\\YangJC\\.cache\\huggingface\\hub\\models--openvla--openvla-7b\\snapshots\\openvla\n",
      "data_root_dir: C:\\Users\\YangJC\\Desktop\\tensorflow_datasets\n",
      "dataset_name: austin_buds_dataset_converted_externally_to_rlds\n",
      "run_root_dir: runs\n",
      "adapter_tmp_dir: adapter-tmp\n",
      "batch_size: 1\n",
      "max_steps: 400\n",
      "save_steps: 200\n",
      "learning_rate: 0.0005\n",
      "grad_accumulation_steps: 1\n",
      "image_aug: True\n",
      "shuffle_buffer_size: 100000\n",
      "save_latest_checkpoint_only: False\n",
      "use_lora: True\n",
      "lora_rank: 32\n",
      "lora_dropout: 0.0\n",
      "use_quantization: True\n",
      "wandb_project: openvla\n",
      "wandb_entity: stanford-voltron\n",
      "run_id_note: None\n"
     ]
    }
   ],
   "source": [
    "cfg = FinetuneConfig(\n",
    "    vla_path=Path(r\"/root/autodl-tmp/openvla/myScripts/runs/huggingface_models+austin_buds_dataset_converted_externally_to_rlds+b2+lr-0.0005+lora-r32+dropout-0.0\"),\n",
    "    data_root_dir=Path(r\"/root/autodl-tmp/modified_libero_rlds/libero_spatial_no_noops\"),\n",
    "    dataset_name='1.0.0',\n",
    "    batch_size=2,\n",
    ")\n",
    "print(\"Fine-tuning with the following configuration:\")\n",
    "for key, value in cfg.__dict__.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "# 你的 fine-tuning 逻辑...\n",
    "# 实例化 FinetuneConfig\n",
    "# [Validate] Ensure GPU Available & Set Device / Distributed Context\n",
    "assert torch.cuda.is_available(), \"Fine-tuning assumes at least one GPU is available!\"\n",
    "distributed_state = PartialState()\n",
    "torch.cuda.set_device(device_id := distributed_state.local_process_index)  #device_id 通过 distributed_state.local_process_index 获得\n",
    "torch.cuda.empty_cache()\n",
    "# Quantization Config =>> only if LoRA fine-tuning\n",
    "quantization_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoConfig.register(\"openvla\", OpenVLAConfig)  \n",
    "# AutoImageProcessor.register(OpenVLAConfig, PrismaticImageProcessor)\n",
    "# AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n",
    "# AutoModelForVision2Seq.register(OpenVLAConfig, OpenVLAForActionPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load OpenVLA Processor and Model using HF AutoClasses\n",
    "processor = AutoProcessor.from_pretrained(cfg.vla_path, trust_remote_code=True) # 加载处理器（Processor）  AutoProcessor\n",
    "vla = AutoModelForVision2Seq.from_pretrained(     # 加载模型（Model） AutoModelForVision2Seq\n",
    "    cfg.vla_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,  #如果启用了量化，则传递quantization_config\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 检查模型的第一个参数所在的设备\n",
    "print(\"Model device:\", next(vla.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrismaticProcessor:\n",
       "- image_processor: PrismaticImageProcessor {\n",
       "  \"auto_map\": {\n",
       "    \"AutoImageProcessor\": \"processing_prismatic.PrismaticImageProcessor\",\n",
       "    \"AutoProcessor\": \"processing_prismatic.PrismaticProcessor\"\n",
       "  },\n",
       "  \"image_processor_type\": \"PrismaticImageProcessor\",\n",
       "  \"image_resize_strategy\": \"resize-naive\",\n",
       "  \"input_sizes\": [\n",
       "    [\n",
       "      3,\n",
       "      224,\n",
       "      224\n",
       "    ],\n",
       "    [\n",
       "      3,\n",
       "      224,\n",
       "      224\n",
       "    ]\n",
       "  ],\n",
       "  \"interpolations\": [\n",
       "    \"bicubic\",\n",
       "    \"bicubic\"\n",
       "  ],\n",
       "  \"means\": [\n",
       "    [\n",
       "      0.485,\n",
       "      0.456,\n",
       "      0.406\n",
       "    ],\n",
       "    [\n",
       "      0.5,\n",
       "      0.5,\n",
       "      0.5\n",
       "    ]\n",
       "  ],\n",
       "  \"processor_class\": \"PrismaticProcessor\",\n",
       "  \"stds\": [\n",
       "    [\n",
       "      0.229,\n",
       "      0.224,\n",
       "      0.225\n",
       "    ],\n",
       "    [\n",
       "      0.5,\n",
       "      0.5,\n",
       "      0.5\n",
       "    ]\n",
       "  ],\n",
       "  \"tvf_crop_params\": [\n",
       "    {\n",
       "      \"output_size\": [\n",
       "        224,\n",
       "        224\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"output_size\": [\n",
       "        224,\n",
       "        224\n",
       "      ]\n",
       "    }\n",
       "  ],\n",
       "  \"tvf_do_letterbox\": false,\n",
       "  \"tvf_letterbox_fill\": null,\n",
       "  \"tvf_normalize_params\": [\n",
       "    {\n",
       "      \"inplace\": false,\n",
       "      \"mean\": [\n",
       "        0.484375,\n",
       "        0.455078125,\n",
       "        0.40625\n",
       "      ],\n",
       "      \"std\": [\n",
       "        0.228515625,\n",
       "        0.2236328125,\n",
       "        0.224609375\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"inplace\": false,\n",
       "      \"mean\": [\n",
       "        0.5,\n",
       "        0.5,\n",
       "        0.5\n",
       "      ],\n",
       "      \"std\": [\n",
       "        0.5,\n",
       "        0.5,\n",
       "        0.5\n",
       "      ]\n",
       "    }\n",
       "  ],\n",
       "  \"tvf_resize_params\": [\n",
       "    {\n",
       "      \"antialias\": true,\n",
       "      \"interpolation\": 3,\n",
       "      \"max_size\": null,\n",
       "      \"size\": [\n",
       "        224,\n",
       "        224\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"antialias\": true,\n",
       "      \"interpolation\": 3,\n",
       "      \"max_size\": null,\n",
       "      \"size\": [\n",
       "        224,\n",
       "        224\n",
       "      ]\n",
       "    }\n",
       "  ],\n",
       "  \"use_fused_vision_backbone\": true\n",
       "}\n",
       "\n",
       "- tokenizer: LlamaTokenizerFast(name_or_path='C:\\Users\\YangJC\\.cache\\huggingface\\hub\\models--openvla--openvla-7b\\snapshots\\openvla', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<PAD>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<PAD>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"PrismaticProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Device Placement =>> note that BitsAndBytes automatically handles for quantized training\n",
    "# if cfg.use_quantization:\n",
    "#     vla = prepare_model_for_kbit_training(vla)\n",
    "# else:\n",
    "#     vla = vla.to(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{device_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "class ActionTokenizer:\n",
    "    def __init__(\n",
    "        self, tokenizer: PreTrainedTokenizerBase, bins: int = 256, min_action: int = -1, max_action: int = 1\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Discretizes continuous robot actions into N bins per dimension and maps to the least used tokens.\n",
    "\n",
    "        NOTE =>> by default, assumes a BPE-style tokenizer akin to the LlamaTokenizer, where *the least used tokens*\n",
    "                 appear at the end of the vocabulary!\n",
    "\n",
    "        :param tokenizer: Base LLM/VLM tokenizer to extend.\n",
    "        :param bins: Number of bins for each continuous value; we'll adopt a uniform binning strategy.\n",
    "        :param min_action: Minimum action value (for clipping, setting lower bound on bin interval).\n",
    "        :param max_action: Maximum action value (for clipping, setting upper bound on bin interval).\n",
    "        \"\"\"\n",
    "        self.tokenizer, self.n_bins, self.min_action, self.max_action = tokenizer, bins, min_action, max_action\n",
    "\n",
    "        # Create Uniform Bins + Compute Bin Centers\n",
    "        self.bins = np.linspace(min_action, max_action, self.n_bins) #创建均匀分布的离散化区间（bins）\n",
    "        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2.0     #创建区间中心（bin_centers）每个区间的中心点\n",
    "\n",
    "        # [Contract] Set \"action_token_begin_idx\" based on `self.tokenizer.vocab_size - (self.n_bins + 1)`\n",
    "        #   =>> Assumes we're always overwriting the final `n_bins` tokens of the vocabulary!\n",
    "        self.action_token_begin_idx: int = int(self.tokenizer.vocab_size - (self.n_bins + 1)) #计算动作对应的分词器词汇表的起始索引，离散化后的动作映射到词汇表的最后n_bins个位置\n",
    "\n",
    "    def __call__(self, action: np.ndarray) -> Union[str, List[str]]: #将输入的连续动作值离散化，并将离散化后的动作索引映射到分词器的词汇表中\n",
    "        \"\"\"Clip & bin actions to *the last `n_bins` tokens* of the vocabulary (e.g., tokenizer.vocab[-256:]).\"\"\"\n",
    "        action = np.clip(action, a_min=float(self.min_action), a_max=float(self.max_action))\n",
    "        discretized_action = np.digitize(action, self.bins)   # 离散化动作为整数索引 action 落在哪个区间（bin）内\n",
    " \n",
    "        # Handle single element vs. batch\n",
    "        if len(discretized_action.shape) == 1:\n",
    "            return self.tokenizer.decode(list(self.tokenizer.vocab_size - discretized_action))# 映射到词表decoded_action对应的字符串\n",
    "        else:\n",
    "            return self.tokenizer.batch_decode((self.tokenizer.vocab_size - discretized_action).tolist())\n",
    "\n",
    "    def decode_token_ids_to_actions(self, action_token_ids: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns continuous actions for discrete action token IDs.\n",
    "\n",
    "        NOTE =>> Because of the way the actions are discretized w.r.t. the bins (and not the bin centers), the\n",
    "                 digitization returns bin indices between [1, # bins], inclusive, when there are actually only\n",
    "                 (# bins - 1) bin intervals.\n",
    "\n",
    "                 Therefore, if the digitization returns the last possible index, we map this to the last bin interval.\n",
    "\n",
    "        EXAMPLE =>> Let's say self._bins has 256 values. Then self._bin_centers has 255 values. Digitization returns\n",
    "                    indices between [1, 256]. We subtract 1 from all indices so that they are between [0, 255]. There\n",
    "                    is still one index (i==255) that would cause an out-of-bounds error if used to index into\n",
    "                    self._bin_centers. Therefore, if i==255, we subtract 1 from it so that it just becomes the index of\n",
    "                    the last bin center. We implement this simply via clipping between [0, 255 - 1].\n",
    "        \"\"\"\n",
    "        discretized_actions = self.tokenizer.vocab_size - action_token_ids\n",
    "        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.bin_centers.shape[0] - 1)\n",
    "\n",
    "        return self.bin_centers[discretized_actions]\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return self.n_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ActionTokenizer at 0x1c3ce637400>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_tokenizer = ActionTokenizer(processor.tokenizer)\n",
    "action_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "എ\n",
      "候\n",
      "麻\n",
      "എ候麻\n"
     ]
    }
   ],
   "source": [
    "action = np.array([0.5])\n",
    "print(action_tokenizer(action))\n",
    "action = np.array([-0.8])\n",
    "print(action_tokenizer(action))\n",
    "action = np.array([0.2])\n",
    "print(action_tokenizer(action))\n",
    "action = np.array([0.5, -0.8, 0.2])\n",
    "print(action_tokenizer(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
