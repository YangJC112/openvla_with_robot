{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuangzhi/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-22 22:24:07.322359: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-22 22:24:07.322423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-22 22:24:07.323649: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-22 22:24:07.329503: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-22 22:24:08.024059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from transformers import AutoConfig, AutoImageProcessor\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from typing import Type, Any\n",
    "from prismatic.extern.hf.configuration_prismatic import OpenVLAConfig\n",
    "from prismatic.extern.hf.modeling_prismatic import OpenVLAForActionPrediction\n",
    "from prismatic.extern.hf.processing_prismatic import PrismaticImageProcessor, PrismaticProcessor\n",
    "from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder\n",
    "from prismatic.util.data_utils import PaddedCollatorForActionPrediction\n",
    "from prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset\n",
    "from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\n",
    "# 注册 OpenVLA 模型到 HF Auto Classes\n",
    "AutoConfig.register(\"openvla\", OpenVLAConfig)\n",
    "AutoImageProcessor.register(OpenVLAConfig, PrismaticImageProcessor)\n",
    "AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n",
    "AutoModelForVision2Seq.register(OpenVLAConfig, OpenVLAForActionPrediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Union, Any\n",
    "import json\n",
    "import os\n",
    "\n",
    "class OpenVLAInference:\n",
    "    def __init__(self, model_path, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        初始化 OpenVLA 推理器\n",
    "        \n",
    "        参数:\n",
    "            model_path: 模型路径 (可以是 HuggingFace Hub 路径或本地路径)\n",
    "            device: 推理设备 ('cuda' 或 'cpu')\n",
    "        \"\"\"\n",
    "        self.device = device if torch.cuda.is_available() and device.startswith(\"cuda\") else \"cpu\"\n",
    "        dataset_statistics_path = os.path.join(model_path, \"dataset_statistics.json\")\n",
    "        if os.path.isfile(dataset_statistics_path):\n",
    "            with open(dataset_statistics_path, \"r\") as f:\n",
    "                norm_stats = json.load(f)\n",
    "            self.norm_stats = norm_stats\n",
    "        self.unnorm_key = \"example_dataset\"\n",
    "        assert self.unnorm_key in self.norm_stats, f\"Action un-norm key {self.unnorm_key} not found in VLA `norm_stats`!\"\n",
    "        # 加载处理器和模型\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForVision2Seq.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        ).to(self.device)\n",
    "        self.model.norm_stats = self.norm_stats\n",
    "        # 创建 action tokenizer\n",
    "        self.action_tokenizer = ActionTokenizer(self.processor.tokenizer)\n",
    "        \n",
    "        # 根据模型版本选择 prompt builder\n",
    "        self.prompt_builder_cls = (\n",
    "            PurePromptBuilder if \"v01\" not in model_path \n",
    "            else VicunaV15ChatPromptBuilder\n",
    "        )\n",
    "        \n",
    "        # 设置模型为评估模式\n",
    "        self.model.eval()\n",
    "    \n",
    "    def _build_prompt(self, text_instruction: str) -> str:\n",
    "        \"\"\"\n",
    "        构建推理时使用的 prompt (直接实现原RLDSBatchTransform的逻辑)\n",
    "        \n",
    "        参数:\n",
    "            text_instruction: 文本指令 (如 \"wipe the table\")\n",
    "            \n",
    "        返回:\n",
    "            格式化后的 prompt 文本\n",
    "        \"\"\"\n",
    "        # 初始化 prompt builder\n",
    "        self.prompt_builder = self.prompt_builder_cls(\"openvla\")\n",
    "        # print(\"self.prompt_builder.turn_count\",self.prompt_builder.turn_count)\n",
    "        conversation = [\n",
    "            {\"from\": \"human\", \"value\":f\"What action should the robot take to {text_instruction.lower()}?\"},\n",
    "            # {\"from\": \"gpt\", \"value\":\"\"},\n",
    "        ]\n",
    "        # 添加对话轮次 (只包含人类指令部分)\n",
    "        for turn in conversation:\n",
    "            self.prompt_builder.add_turn(turn[\"from\"], turn[\"value\"])\n",
    "        # print(\"self.prompt_builder.turn_count\",self.prompt_builder.turn_count)\n",
    "        self.turn_count = self.prompt_builder.turn_count\n",
    "        return self.prompt_builder.get_prompt()\n",
    "    \n",
    "    def preprocess_inputs(self, text_prompt: str, image: Any) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        预处理输入文本和图像\n",
    "        \n",
    "        参数:\n",
    "            text_prompt: 文本指令\n",
    "            image: PIL.Image 对象或图像路径\n",
    "            \n",
    "        返回:\n",
    "            处理后的模型输入字典\n",
    "        \"\"\"\n",
    "        # 如果 image 是路径，则加载图像\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "        \n",
    "        # 构建 prompt\n",
    "        prompt_text = self._build_prompt(text_prompt)\n",
    "        self.input_ids = self.processor.tokenizer(prompt_text, add_special_tokens=True).input_ids\n",
    "        self.input_ids = torch.tensor(self.input_ids)\n",
    "        # 使用处理器处理输入\n",
    "        inputs = self.processor(\n",
    "            text=prompt_text,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # 将输入移动到设备\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def generate_actions(self, text_prompt: str, image: Any, \n",
    "                       max_new_tokens: int = 512, \n",
    "                       temperature: float = 0) -> tuple:\n",
    "        \"\"\"\n",
    "        生成动作序列\n",
    "        \n",
    "        参数:\n",
    "            text_prompt: 文本指令\n",
    "            image: PIL.Image 对象或图像路径\n",
    "            max_new_tokens: 最大生成 token 数\n",
    "            temperature: 采样温度\n",
    "            \n",
    "        返回:\n",
    "            tuple: (action_sequence, decoded_actions)\n",
    "                - action_sequence: 动作序列 (numpy 数组)\n",
    "                - decoded_actions: 解码后的动作 (人类可读格式)\n",
    "        \"\"\"\n",
    "        # 预处理输入\n",
    "        inputs = self.preprocess_inputs(text_prompt, image)\n",
    "        \n",
    "        # 生成动作 token\n",
    "        with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            self.output = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True if temperature > 0 else False,\n",
    "                pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "            )\n",
    "            self.action = self.model.predict_action(**inputs,\n",
    "                                               unnorm_key=self.unnorm_key,\n",
    "                                               temperature=temperature,\n",
    "                                               do_sample=True if temperature > 0 else False,\n",
    "                                               )\n",
    "        mask = self.output > self.action_tokenizer.action_token_begin_idx\n",
    "        # 解码动作 token\n",
    "        action_tokens = self.output[mask].cpu().numpy()\n",
    "        action_sequence = self.action_tokenizer.decode_token_ids_to_actions(action_tokens)\n",
    "        \n",
    "        # 获取人类可读的动作描述\n",
    "        # decoded_actions = self.action_tokenizer.decode_actions_to_readable(action_sequence)\n",
    "        \n",
    "        return action_sequence\n",
    "    \n",
    "    def __call__(self, text_prompt: str, image: Any, \n",
    "                max_new_tokens: int = 512, \n",
    "                temperature: float = 0) -> tuple:\n",
    "        \"\"\"便捷调用方法\"\"\"\n",
    "        return self.generate_actions(text_prompt, image, max_new_tokens, temperature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('cuda:2', dict_keys(['example_dataset']))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 使用示例\n",
    "# !CUDA_VISIBLE_DEVICES=1\n",
    "vla_path = r\"/home/chuangzhi/zhq/yjc/runs/openvla7b_huggingfacemodel+libero_spatial_no_noops+b2+lr-0.0005+lora-r32+dropout-0.0+example_dataset+b1+lr-0.0005+lora-r32+dropout-0.0\"\n",
    "# vla_path = r\"/home/chuangzhi/zhq/yjc/runs/openvla7b_huggingfacemodel+libero_spatial_no_noops+b2+lr-0.0005+lora-r32+dropout-0.0\"\n",
    "# 初始化推理器 - 替换为你的模型路径  # 可以是本地路径或 HuggingFace Hub 路径\n",
    "vla_inference = OpenVLAInference(vla_path,device=\"cuda:2\")\n",
    "vla_inference.device, vla_inference.norm_stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuangzhi/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use .generate(), Generated Action Sequence(norm): [0.         0.23529412 0.90980392 0.03137255 0.02352941 0.21176471\n",
      " 0.99607843]\n",
      "\"action(去归一化前)\" 序列长度为:7\n",
      "use .predict_action(), Generated Action Sequence(unnorm): [-5.52274287e-06 -9.16986781e-05  3.00139446e-02  7.77396050e-04\n",
      " -8.98995658e-06  8.26287587e-04  9.96078431e-01]\n"
     ]
    }
   ],
   "source": [
    "# 示例输入\n",
    "text_instruction = \"Move the drinking glass to the basket.\"  # 注意使用小写，与训练时一致\n",
    "image_path = \"/home/chuangzhi/zhq/yjc/myScripts/9.png\"  # 你的图像路径\n",
    "\n",
    "# 生成动作\n",
    "action_sequence = vla_inference(text_instruction, image_path)\n",
    "\n",
    "# 打印结果\n",
    "print(\"use .generate(), Generated Action Sequence(norm):\", action_sequence)\n",
    "print(f'\"action(去归一化前)\" 序列长度为:{len(action_sequence)}')\n",
    "print(\"use .predict_action(), Generated Action Sequence(unnorm):\", vla_inference.action)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use .generate(), Generated Action Sequence(norm): [0.90980392 0.24313725 0.         0.09411765 0.10980392 0.19607843\n",
    " 0.99607843]\n",
    "\n",
    "\"action(去归一化前)\" 序列长度为:7\n",
    "\n",
    "[3.00022413e-02 1.17139460e-04 2.68705189e-05 1.69970930e-03 2.35824911e-06 6.13276555e-04 9.96078431e-01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例输入\n",
    "text_instruction = \"Move the drinking glass to the basket.\"  # 注意使用小写，与训练时一致\n",
    "image_path = \"/home/chuangzhi/zhq/yjc/myScripts/0.png\"  # 你的图像路径\n",
    "\n",
    "# 生成动作\n",
    "action_sequence = vla_inference(text_instruction, image_path)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Generated Action Sequence:\", action_sequence)\n",
    "print(f'\"action\" 序列长度为:{len(action_sequence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Form data requires \"python-multipart\" to be installed. \n",
      "You can install \"python-multipart\" with: \n",
      "\n",
      "pip install python-multipart\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Form data requires \"python-multipart\" to be installed. \nYou can install \"python-multipart\" with: \n\npip install python-multipart\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m@app\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m/healthcheck\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mhealthcheck\u001b[39m():\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mOK\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     10\u001b[0m \u001b[39m@app\u001b[39;49m\u001b[39m.\u001b[39;49mpost(\u001b[39m\"\u001b[39;49m\u001b[39m/generate_action\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m---> 11\u001b[0m \u001b[39masync\u001b[39;49;00m \u001b[39mdef\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39mgenerate_action\u001b[39;49m(instruction: \u001b[39mstr\u001b[39;49m, image: UploadFile \u001b[39m=\u001b[39;49m File(\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m)):\n\u001b[1;32m     12\u001b[0m     \u001b[39m# TODO: 调用你的模型\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;49;00m {\u001b[39m\"\u001b[39;49m\u001b[39maction\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m0.1\u001b[39;49m, \u001b[39m0.2\u001b[39;49m, \u001b[39m0.3\u001b[39;49m]}  \u001b[39m# 示例返回\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/fastapi/routing.py:994\u001b[0m, in \u001b[0;36mAPIRouter.api_route.<locals>.decorator\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorator\u001b[39m(func: DecoratedCallable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DecoratedCallable:\n\u001b[0;32m--> 994\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_api_route(\n\u001b[1;32m    995\u001b[0m         path,\n\u001b[1;32m    996\u001b[0m         func,\n\u001b[1;32m    997\u001b[0m         response_model\u001b[39m=\u001b[39;49mresponse_model,\n\u001b[1;32m    998\u001b[0m         status_code\u001b[39m=\u001b[39;49mstatus_code,\n\u001b[1;32m    999\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m   1000\u001b[0m         dependencies\u001b[39m=\u001b[39;49mdependencies,\n\u001b[1;32m   1001\u001b[0m         summary\u001b[39m=\u001b[39;49msummary,\n\u001b[1;32m   1002\u001b[0m         description\u001b[39m=\u001b[39;49mdescription,\n\u001b[1;32m   1003\u001b[0m         response_description\u001b[39m=\u001b[39;49mresponse_description,\n\u001b[1;32m   1004\u001b[0m         responses\u001b[39m=\u001b[39;49mresponses,\n\u001b[1;32m   1005\u001b[0m         deprecated\u001b[39m=\u001b[39;49mdeprecated,\n\u001b[1;32m   1006\u001b[0m         methods\u001b[39m=\u001b[39;49mmethods,\n\u001b[1;32m   1007\u001b[0m         operation_id\u001b[39m=\u001b[39;49moperation_id,\n\u001b[1;32m   1008\u001b[0m         response_model_include\u001b[39m=\u001b[39;49mresponse_model_include,\n\u001b[1;32m   1009\u001b[0m         response_model_exclude\u001b[39m=\u001b[39;49mresponse_model_exclude,\n\u001b[1;32m   1010\u001b[0m         response_model_by_alias\u001b[39m=\u001b[39;49mresponse_model_by_alias,\n\u001b[1;32m   1011\u001b[0m         response_model_exclude_unset\u001b[39m=\u001b[39;49mresponse_model_exclude_unset,\n\u001b[1;32m   1012\u001b[0m         response_model_exclude_defaults\u001b[39m=\u001b[39;49mresponse_model_exclude_defaults,\n\u001b[1;32m   1013\u001b[0m         response_model_exclude_none\u001b[39m=\u001b[39;49mresponse_model_exclude_none,\n\u001b[1;32m   1014\u001b[0m         include_in_schema\u001b[39m=\u001b[39;49minclude_in_schema,\n\u001b[1;32m   1015\u001b[0m         response_class\u001b[39m=\u001b[39;49mresponse_class,\n\u001b[1;32m   1016\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1017\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1018\u001b[0m         openapi_extra\u001b[39m=\u001b[39;49mopenapi_extra,\n\u001b[1;32m   1019\u001b[0m         generate_unique_id_function\u001b[39m=\u001b[39;49mgenerate_unique_id_function,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/fastapi/routing.py:933\u001b[0m, in \u001b[0;36mAPIRouter.add_api_route\u001b[0;34m(self, path, endpoint, response_model, status_code, tags, dependencies, summary, description, response_description, responses, deprecated, methods, operation_id, response_model_include, response_model_exclude, response_model_by_alias, response_model_exclude_unset, response_model_exclude_defaults, response_model_exclude_none, include_in_schema, response_class, name, route_class_override, callbacks, openapi_extra, generate_unique_id_function)\u001b[0m\n\u001b[1;32m    929\u001b[0m     current_callbacks\u001b[39m.\u001b[39mextend(callbacks)\n\u001b[1;32m    930\u001b[0m current_generate_unique_id \u001b[39m=\u001b[39m get_value_or_default(\n\u001b[1;32m    931\u001b[0m     generate_unique_id_function, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_unique_id_function\n\u001b[1;32m    932\u001b[0m )\n\u001b[0;32m--> 933\u001b[0m route \u001b[39m=\u001b[39m route_class(\n\u001b[1;32m    934\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprefix \u001b[39m+\u001b[39;49m path,\n\u001b[1;32m    935\u001b[0m     endpoint\u001b[39m=\u001b[39;49mendpoint,\n\u001b[1;32m    936\u001b[0m     response_model\u001b[39m=\u001b[39;49mresponse_model,\n\u001b[1;32m    937\u001b[0m     status_code\u001b[39m=\u001b[39;49mstatus_code,\n\u001b[1;32m    938\u001b[0m     tags\u001b[39m=\u001b[39;49mcurrent_tags,\n\u001b[1;32m    939\u001b[0m     dependencies\u001b[39m=\u001b[39;49mcurrent_dependencies,\n\u001b[1;32m    940\u001b[0m     summary\u001b[39m=\u001b[39;49msummary,\n\u001b[1;32m    941\u001b[0m     description\u001b[39m=\u001b[39;49mdescription,\n\u001b[1;32m    942\u001b[0m     response_description\u001b[39m=\u001b[39;49mresponse_description,\n\u001b[1;32m    943\u001b[0m     responses\u001b[39m=\u001b[39;49mcombined_responses,\n\u001b[1;32m    944\u001b[0m     deprecated\u001b[39m=\u001b[39;49mdeprecated \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeprecated,\n\u001b[1;32m    945\u001b[0m     methods\u001b[39m=\u001b[39;49mmethods,\n\u001b[1;32m    946\u001b[0m     operation_id\u001b[39m=\u001b[39;49moperation_id,\n\u001b[1;32m    947\u001b[0m     response_model_include\u001b[39m=\u001b[39;49mresponse_model_include,\n\u001b[1;32m    948\u001b[0m     response_model_exclude\u001b[39m=\u001b[39;49mresponse_model_exclude,\n\u001b[1;32m    949\u001b[0m     response_model_by_alias\u001b[39m=\u001b[39;49mresponse_model_by_alias,\n\u001b[1;32m    950\u001b[0m     response_model_exclude_unset\u001b[39m=\u001b[39;49mresponse_model_exclude_unset,\n\u001b[1;32m    951\u001b[0m     response_model_exclude_defaults\u001b[39m=\u001b[39;49mresponse_model_exclude_defaults,\n\u001b[1;32m    952\u001b[0m     response_model_exclude_none\u001b[39m=\u001b[39;49mresponse_model_exclude_none,\n\u001b[1;32m    953\u001b[0m     include_in_schema\u001b[39m=\u001b[39;49minclude_in_schema \u001b[39mand\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minclude_in_schema,\n\u001b[1;32m    954\u001b[0m     response_class\u001b[39m=\u001b[39;49mcurrent_response_class,\n\u001b[1;32m    955\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    956\u001b[0m     dependency_overrides_provider\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdependency_overrides_provider,\n\u001b[1;32m    957\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcurrent_callbacks,\n\u001b[1;32m    958\u001b[0m     openapi_extra\u001b[39m=\u001b[39;49mopenapi_extra,\n\u001b[1;32m    959\u001b[0m     generate_unique_id_function\u001b[39m=\u001b[39;49mcurrent_generate_unique_id,\n\u001b[1;32m    960\u001b[0m )\n\u001b[1;32m    961\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroutes\u001b[39m.\u001b[39mappend(route)\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/fastapi/routing.py:554\u001b[0m, in \u001b[0;36mAPIRoute.__init__\u001b[0;34m(self, path, endpoint, response_model, status_code, tags, dependencies, summary, description, response_description, responses, deprecated, name, methods, operation_id, response_model_include, response_model_exclude, response_model_by_alias, response_model_exclude_unset, response_model_exclude_defaults, response_model_exclude_none, include_in_schema, response_class, dependency_overrides_provider, callbacks, openapi_extra, generate_unique_id_function)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_fields \u001b[39m=\u001b[39m {}\n\u001b[1;32m    553\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(endpoint), \u001b[39m\"\u001b[39m\u001b[39mAn endpoint must be a callable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 554\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdependant \u001b[39m=\u001b[39m get_dependant(path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath_format, call\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint)\n\u001b[1;32m    555\u001b[0m \u001b[39mfor\u001b[39;00m depends \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdependencies[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m    556\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdependant\u001b[39m.\u001b[39mdependencies\u001b[39m.\u001b[39minsert(\n\u001b[1;32m    557\u001b[0m         \u001b[39m0\u001b[39m,\n\u001b[1;32m    558\u001b[0m         get_parameterless_sub_dependant(depends\u001b[39m=\u001b[39mdepends, path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_format),\n\u001b[1;32m    559\u001b[0m     )\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/fastapi/dependencies/utils.py:285\u001b[0m, in \u001b[0;36mget_dependant\u001b[0;34m(path, call, name, security_scopes, use_cache)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mfor\u001b[39;00m param_name, param \u001b[39min\u001b[39;00m signature_params\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    284\u001b[0m     is_path_param \u001b[39m=\u001b[39m param_name \u001b[39min\u001b[39;00m path_param_names\n\u001b[0;32m--> 285\u001b[0m     param_details \u001b[39m=\u001b[39m analyze_param(\n\u001b[1;32m    286\u001b[0m         param_name\u001b[39m=\u001b[39;49mparam_name,\n\u001b[1;32m    287\u001b[0m         annotation\u001b[39m=\u001b[39;49mparam\u001b[39m.\u001b[39;49mannotation,\n\u001b[1;32m    288\u001b[0m         value\u001b[39m=\u001b[39;49mparam\u001b[39m.\u001b[39;49mdefault,\n\u001b[1;32m    289\u001b[0m         is_path_param\u001b[39m=\u001b[39;49mis_path_param,\n\u001b[1;32m    290\u001b[0m     )\n\u001b[1;32m    291\u001b[0m     \u001b[39mif\u001b[39;00m param_details\u001b[39m.\u001b[39mdepends \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m         sub_dependant \u001b[39m=\u001b[39m get_param_sub_dependant(\n\u001b[1;32m    293\u001b[0m             param_name\u001b[39m=\u001b[39mparam_name,\n\u001b[1;32m    294\u001b[0m             depends\u001b[39m=\u001b[39mparam_details\u001b[39m.\u001b[39mdepends,\n\u001b[1;32m    295\u001b[0m             path\u001b[39m=\u001b[39mpath,\n\u001b[1;32m    296\u001b[0m             security_scopes\u001b[39m=\u001b[39msecurity_scopes,\n\u001b[1;32m    297\u001b[0m         )\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/fastapi/dependencies/utils.py:482\u001b[0m, in \u001b[0;36manalyze_param\u001b[0;34m(param_name, annotation, value, is_path_param)\u001b[0m\n\u001b[1;32m    476\u001b[0m use_annotation_from_field_info \u001b[39m=\u001b[39m get_annotation_from_field_info(\n\u001b[1;32m    477\u001b[0m     use_annotation,\n\u001b[1;32m    478\u001b[0m     field_info,\n\u001b[1;32m    479\u001b[0m     param_name,\n\u001b[1;32m    480\u001b[0m )\n\u001b[1;32m    481\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(field_info, params\u001b[39m.\u001b[39mForm):\n\u001b[0;32m--> 482\u001b[0m     ensure_multipart_is_installed()\n\u001b[1;32m    483\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m field_info\u001b[39m.\u001b[39malias \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(field_info, \u001b[39m\"\u001b[39m\u001b[39mconvert_underscores\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    484\u001b[0m     alias \u001b[39m=\u001b[39m param_name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/fastapi/dependencies/utils.py:115\u001b[0m, in \u001b[0;36mensure_multipart_is_installed\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     logger\u001b[39m.\u001b[39merror(multipart_not_installed_error)\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(multipart_not_installed_error) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Form data requires \"python-multipart\" to be installed. \nYou can install \"python-multipart\" with: \n\npip install python-multipart\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/healthcheck\")\n",
    "def healthcheck():\n",
    "    return {\"status\": \"OK\"}\n",
    "\n",
    "@app.post(\"/generate_action\")\n",
    "async def generate_action(instruction: str, image: UploadFile = File(...)):\n",
    "    # TODO: 调用你的模型\n",
    "    return {\"action\": [0.1, 0.2, 0.3]}  # 示例返回\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IncompatibleProtocolError",
     "evalue": "StreamLostError: (\"Stream connection lost: ConnectionResetError(104, 'Connection reset by peer')\",)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIncompatibleProtocolError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpika\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m connection \u001b[39m=\u001b[39m pika\u001b[39m.\u001b[39;49mBlockingConnection(pika\u001b[39m.\u001b[39;49mConnectionParameters(\u001b[39m'\u001b[39;49m\u001b[39mlocalhost\u001b[39;49m\u001b[39m'\u001b[39;49m, port\u001b[39m=\u001b[39;49m\u001b[39m9025\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m channel \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mchannel()\n\u001b[1;32m      7\u001b[0m channel\u001b[39m.\u001b[39mqueue_declare(queue\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maction_requests\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/pika/adapters/blocking_connection.py:360\u001b[0m, in \u001b[0;36mBlockingConnection.__init__\u001b[0;34m(self, parameters, _impl_class)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39m# Perform connection workflow\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# so that attribute is created in case below raises\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection(parameters, _impl_class)\n\u001b[1;32m    361\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl\u001b[39m.\u001b[39madd_on_close_callback(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed_result\u001b[39m.\u001b[39mset_value_once)\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/site-packages/pika/adapters/blocking_connection.py:451\u001b[0m, in \u001b[0;36mBlockingConnection._create_connection\u001b[0;34m(self, configs, impl_class)\u001b[0m\n\u001b[1;32m    449\u001b[0m     error \u001b[39m=\u001b[39m on_cw_done_result\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mresult\n\u001b[1;32m    450\u001b[0m     LOGGER\u001b[39m.\u001b[39merror(\u001b[39m'\u001b[39m\u001b[39mConnection workflow failed: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m, error)\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reap_last_connection_workflow_error(error)\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mConnection workflow succeeded: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m    454\u001b[0m                 on_cw_done_result\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mresult)\n",
      "\u001b[0;31mIncompatibleProtocolError\u001b[0m: StreamLostError: (\"Stream connection lost: ConnectionResetError(104, 'Connection reset by peer')\",)"
     ]
    }
   ],
   "source": [
    "import pika\n",
    "import json\n",
    "\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost', port=9025))\n",
    "channel = connection.channel()\n",
    "\n",
    "channel.queue_declare(queue='action_requests')\n",
    "channel.queue_declare(queue='action_responses')\n",
    "\n",
    "def callback(ch, method, properties, body):\n",
    "    request = json.loads(body)\n",
    "    action = vla_inference(request['instruction'], request['image_path'])\n",
    "    \n",
    "    response = {\n",
    "        'action': vla_inference.action,\n",
    "        'request_id': request['request_id']\n",
    "    }\n",
    "    \n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='action_responses',\n",
    "        body=json.dumps(response)\n",
    "    )\n",
    "\n",
    "channel.basic_consume(queue='action_requests', on_message_callback=callback, auto_ack=True)\n",
    "channel.start_consuming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 98] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m server_port \u001b[39m=\u001b[39m \u001b[39m9025\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mwith\u001b[39;00m socket\u001b[39m.\u001b[39msocket(socket\u001b[39m.\u001b[39mAF_INET, socket\u001b[39m.\u001b[39mSOCK_STREAM) \u001b[39mas\u001b[39;00m sock:\n\u001b[0;32m----> 8\u001b[0m     sock\u001b[39m.\u001b[39;49mbind((server_ip, server_port))\n\u001b[1;32m      9\u001b[0m     sock\u001b[39m.\u001b[39mlisten(\u001b[39m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mServer is listening on port 9025\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 98] Address already in use"
     ]
    }
   ],
   "source": [
    "# 远程服务器代码\n",
    "import socket\n",
    "\n",
    "server_ip = \"0.0.0.0\"  # 监听所有IP\n",
    "server_port = 9025\n",
    "\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "    sock.bind((server_ip, server_port))\n",
    "    sock.listen(1)\n",
    "    print(\"Server is listening on port 9025\")\n",
    "    conn, addr = sock.accept()\n",
    "    with conn:\n",
    "        print(f\"Connected by {addr}\")\n",
    "        data = conn.recv(1024)\n",
    "        print(f\"Received from client: {data.decode()}\")\n",
    "        conn.sendall(b\"Hello from server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on port 6789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m s\u001b[39m.\u001b[39mlisten()\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mServer listening on port \u001b[39m\u001b[39m{\u001b[39;00mport\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m conn, addr \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39;49maccept()\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m conn:\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConnected by \u001b[39m\u001b[39m{\u001b[39;00maddr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/socket.py:293\u001b[0m, in \u001b[0;36msocket.accept\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39maccept\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    287\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"accept() -> (socket object, address info)\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[39m    Wait for an incoming connection.  Return a new socket\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    representing the connection, and the address of the client.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m    For IP sockets, the address info is a pair (hostaddr, port).\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     fd, addr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accept()\n\u001b[1;32m    294\u001b[0m     sock \u001b[39m=\u001b[39m socket(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfamily, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto, fileno\u001b[39m=\u001b[39mfd)\n\u001b[1;32m    295\u001b[0m     \u001b[39m# Issue #7995: if no default timeout is set and the listening\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[39m# socket had a (non-zero) timeout, force the new socket in blocking\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[39m# mode to override platform-specific socket flags inheritance.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "host = '0.0.0.0'  # 监听所有网络接口\n",
    "port = 6789  # 确保端口号与客户端一致\n",
    "\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    s.bind((host, port))\n",
    "    s.listen()\n",
    "    print(f\"Server listening on port {port}\")\n",
    "    conn, addr = s.accept()\n",
    "    with conn:\n",
    "        print(f\"Connected by {addr}\")\n",
    "        while True:\n",
    "            data = conn.recv(1024)\n",
    "            if not data:\n",
    "                break\n",
    "            print(f\"Received data: {data}\")\n",
    "            conn.sendall(b\"Hello, client!\")  # 发送固定响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server listening on port 8000\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mServer listening on port \u001b[39m\u001b[39m{\u001b[39;00mport\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     conn, addr \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39;49maccept()\n\u001b[1;32m     14\u001b[0m     \u001b[39mwith\u001b[39;00m conn:\n\u001b[1;32m     15\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConnected by \u001b[39m\u001b[39m{\u001b[39;00maddr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/socket.py:293\u001b[0m, in \u001b[0;36msocket.accept\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39maccept\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    287\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"accept() -> (socket object, address info)\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[39m    Wait for an incoming connection.  Return a new socket\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    representing the connection, and the address of the client.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m    For IP sockets, the address info is a pair (hostaddr, port).\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     fd, addr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accept()\n\u001b[1;32m    294\u001b[0m     sock \u001b[39m=\u001b[39m socket(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfamily, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto, fileno\u001b[39m=\u001b[39mfd)\n\u001b[1;32m    295\u001b[0m     \u001b[39m# Issue #7995: if no default timeout is set and the listening\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[39m# socket had a (non-zero) timeout, force the new socket in blocking\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[39m# mode to override platform-specific socket flags inheritance.\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: timed out"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "host = '0.0.0.0'  # 监听所有网络接口\n",
    "port = 8000  # 确保端口号与客户端一致\n",
    "\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    s.bind((host, port))\n",
    "    s.listen()\n",
    "    s.settimeout(10)  # 设置超时时间为10秒\n",
    "    print(f\"Server listening on port {port}\")\n",
    "    \n",
    "    while True:\n",
    "        conn, addr = s.accept()\n",
    "        with conn:\n",
    "            print(f\"Connected by {addr}\")\n",
    "            while True:\n",
    "                data = conn.recv(1024)\n",
    "                if not data:\n",
    "                    print(\"No data received, closing connection.\")\n",
    "                    break\n",
    "                print(f\"Received data: {data.decode()}\")\n",
    "                response = f\"Server received: {data.decode()}\"\n",
    "                print(f\"Sending response: {response}\")\n",
    "                conn.sendall(response.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "host = '0.0.0.0'\n",
    "port = 8000\n",
    "\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    s.bind((host, port))\n",
    "    s.listen()\n",
    "    print(f\"Server listening on port {port}\")\n",
    "    \n",
    "    conn, addr = s.accept()\n",
    "    with conn:\n",
    "        print(f\"Connected by {addr}\")\n",
    "        data = conn.recv(1024)\n",
    "        print(f\"Received data: {data.decode()}\")\n",
    "        response = \"Hello from server\"\n",
    "        conn.sendall(response.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is listening for incoming connections...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mServer is listening for incoming connections...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# 接受客户端连接\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m client_socket, client_address \u001b[39m=\u001b[39m server_socket\u001b[39m.\u001b[39;49maccept()\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConnection from \u001b[39m\u001b[39m{\u001b[39;00mclient_address\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39m# 接收数据\u001b[39;00m\n",
      "File \u001b[0;32m~/zhq/miniconda3/envs/openvla/lib/python3.10/socket.py:293\u001b[0m, in \u001b[0;36msocket.accept\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39maccept\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    287\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"accept() -> (socket object, address info)\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[39m    Wait for an incoming connection.  Return a new socket\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    representing the connection, and the address of the client.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m    For IP sockets, the address info is a pair (hostaddr, port).\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     fd, addr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accept()\n\u001b[1;32m    294\u001b[0m     sock \u001b[39m=\u001b[39m socket(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfamily, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto, fileno\u001b[39m=\u001b[39mfd)\n\u001b[1;32m    295\u001b[0m     \u001b[39m# Issue #7995: if no default timeout is set and the listening\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[39m# socket had a (non-zero) timeout, force the new socket in blocking\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[39m# mode to override platform-specific socket flags inheritance.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import pickle\n",
    "\n",
    "# 创建TCP socket\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind(('0.0.0.0', 12345))  # 监听所有接口，端口为12345\n",
    "server_socket.listen(1)\n",
    "print(\"Server is listening for incoming connections...\")\n",
    "\n",
    "# 接受客户端连接\n",
    "client_socket, client_address = server_socket.accept()\n",
    "print(f\"Connection from {client_address}\")\n",
    "\n",
    "# 接收数据\n",
    "data = client_socket.recv(4096)  # 根据字典大小调整接收缓冲区大小\n",
    "if data:\n",
    "    # 反序列化字典\n",
    "    received_dict = pickle.loads(data)\n",
    "    print(\"Received dictionary:\", received_dict)\n",
    "\n",
    "client_socket.close()\n",
    "server_socket.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = vla_inference.preprocess_inputs(text_instruction, img)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla_inference.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys(),inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla_inference._build_prompt(text_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    output = vla_inference.model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=vla_inference.processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=vla_inference.processor.tokenizer.eos_token_id,\n",
    "    )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = output > vla_inference.action_tokenizer.action_token_begin_idx\n",
    "\n",
    "output[mask].cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
